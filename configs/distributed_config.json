{
    "training": {
        "batch_size": {
            "primary": 32,
            "secondary": 16
        },
        "gradient_accumulation_steps": 16,
        "max_memory_gb": {
            "primary": 160,
            "secondary": 96
        },
        "prefetch_batches": 2,
        "mixed_precision": true,
        "communication": {
            "sync_frequency": 100,
            "gradient_compression": true,
            "tcp_links": 4
        }
    },
    "model": {
        "num_layers": 24,
        "dims": 2048,
        "num_heads": 16,
        "vocab_size": 32000,
        "max_seq_length": 4096,
        "attention": {
            "sliding_window": 512,
            "use_flash": true,
            "kv_heads": 4
        }
    }
} 